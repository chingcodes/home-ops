---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app llama-cpp
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      llama-cpp:
        containers:
          app:
            image:
              repository: ghcr.io/ggerganov/llama.cpp
              tag: server-rocm
            args:
              - --host
              - "0.0.0.0"
              - --port
              - "8080"
              - --model
              - "/models/model.gguf"
              - --n-gpu-layers
              - "99"
              - --ctx-size
              - "4096"
              - --parallel
              - "4"
            env:
              # ROCm-specific environment variables
              HSA_OVERRIDE_GFX_VERSION: "11.0.0"
              HIP_VISIBLE_DEVICES: "0"
              GPU_MAX_HW_QUEUES: "2"
              GGML_CUDA_ENABLE_UNIFIED_MEMORY: 1
            resources:
              requests:
                cpu: 2000m
                memory: 4Gi
                amd.com/gpu: "1"
              limits:
                cpu: 4000m
                memory: 8Gi
                amd.com/gpu: "1"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8080
                  periodSeconds: 10
                  failureThreshold: 3
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8080
                  periodSeconds: 10
                  failureThreshold: 3
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  failureThreshold: 12

    service:
      app:
        controller: llama-cpp
        ports:
          http:
            port: 8080

    persistence:
      models:
        enabled: true
        storageClass: openebs-hostpath
        accessMode: ReadWriteOnce
        size: 20Gi
        globalMounts:
          - path: /models
      dev-dri:
        type: hostPath
        hostPath: /dev/dri
        globalMounts:
          - path: /dev/dri
      dev-kfd:
        type: hostPath
        hostPath: /dev/kfd
        globalMounts:
          - path: /dev/kfd
